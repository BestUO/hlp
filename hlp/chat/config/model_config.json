{
  "seq2seq": {
    "train_data": "checkpoint\\seq2seq"
  },
  "transformer": {
    "train_data": "checkpoint\\transformer",
    "num_layers": 2,
    "d_model": 256,
    "num_heads": 8,
    "units": 512,
    "dropout": 0.1
  },
  "resource_data": "data\\train.txt",
  "tokenized_data": "data\\train_tokenized.txt",
  "enc_vocab_size": 20000,
  "dec_vocab_size": 20000,
  "embedding_dim": 256,
  "layer_size": 1024,
  "max_train_data_size": 100,
  "batch_size": 64,
  "buffer_size": 20000,
  "epochs": 1
}