{
  "en_tokenize_type": "BPE",
  "ch_tokenize_type": "WORD",
  "path_to_train_file": "./data/en-zh_train.txt",
  "path_to_train_file_en": "./data/train_en.txt",
  "path_to_train_file_zh": "./data/train_zh.txt",
  "path_to_eval_file": "./data/en-zh_eval.txt",
  "path_encoded_sequences_en": "./data/encoded_sequences_en.npy",
  "path_encoded_sequences_zh": "./data/encoded_sequences_zh.npy",
  "num_eval": 100,
  "checkpoint_path": "./checkpoints/train",
  "en_bpe_tokenizer_path": "./data/en_tokenizer",
  "en_tokenizer_path": "./data/en_tokenizer.json",
  "ch_tokenizer_path": "./data/ch_tokenizer.json",
  "BUFFER_SIZE": 20000,
  "test_size": 0.001,
  "num_sentences": 1000,
  "BATCH_SIZE": 32,
  "num_layers": 4,
  "d_model": 256,
  "dff": 512,
  "num_heads": 8,
  "dropout_rate" : 0.1,
  "EPOCHS": 10,
  "max_target_length": 200,
  "start_word": "<start>",
  "end_word": "<end>",
  "target_vocab_size": 8192,
  "BEAM_SIZE": 3
}