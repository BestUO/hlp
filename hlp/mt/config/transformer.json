{
  "en_tokenize_type": "BPE",
  "ch_tokenize_type": "TOKENIZE",
  "path_to_train_file": "./data/en-ch.txt",
  "path_to_train_file_en": "./data/train_en.txt",
  "path_to_train_file_zh": "./data/train_zh.txt",
  "path_to_eval_file": "./data/en-ch_eval.txt",
  "num_eval": 10,
  "checkpoint_path": "./checkpoints/train",
  "en_bpe_tokenizer_path": "./data/en_tokenizer",
  "en_tokenizer_path": "./data/en_tokenizer.json",
  "ch_tokenizer_path": "./data/ch_tokenizer.json",
  "BUFFER_SIZE": 20000,
  "BATCH_SIZE": 64,
  "test_size": 0.1,
  "num_sentences": 10000,
  "num_layers": 6,
  "d_model": 512,
  "dff": 2048,
  "num_heads": 8,
  "dropout_rate" : 0.1,
  "EPOCHS": 10,
  "max_target_length": 20,
  "start_word": "<start>",
  "end_word": "<end>",
  "target_vocab_size": 8192,
  "BEAM_SIZE": 3
}